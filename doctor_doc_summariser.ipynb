{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOGcV1czOMFmFP7iAHPo7kT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Khushmeet-patil/Patient_report_summariser/blob/main/doctor_doc_summariser.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JgFIDsPTaOAh",
        "outputId": "e9fe1836-a2e8-49e1-a49c-b32f0ec0ac78",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: flask in /usr/local/lib/python3.10/dist-packages (2.2.5)\n",
            "Collecting pymongo\n",
            "  Downloading pymongo-4.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (22 kB)\n",
            "Collecting ngrok\n",
            "  Downloading ngrok-1.4.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.2.1-py3-none-any.whl.metadata (8.3 kB)\n",
            "Collecting langchain_groq\n",
            "  Downloading langchain_groq-0.2.1-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting streamlit\n",
            "  Downloading streamlit-1.40.0-py2.py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting langchain_community\n",
            "  Downloading langchain_community-0.3.5-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting chromadb\n",
            "  Downloading chromadb-0.5.18-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from flask) (3.0.6)\n",
            "Requirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from flask) (3.1.4)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from flask) (2.2.0)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from flask) (8.1.7)\n",
            "Collecting dnspython<3.0.0,>=1.16.0 (from pymongo)\n",
            "  Downloading dnspython-2.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.2)\n",
            "Collecting groq<1,>=0.4.1 (from langchain_groq)\n",
            "  Downloading groq-0.11.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting langchain-core<0.4.0,>=0.3.15 (from langchain_groq)\n",
            "  Downloading langchain_core-0.3.15-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.2.2)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/lib/python3/dist-packages (from streamlit) (1.4)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: numpy<3,>=1.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.26.4)\n",
            "Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (24.1)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (10.4.0)\n",
            "Requirement already satisfied: protobuf<6,>=3.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.20.3)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (17.0.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (13.9.3)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (9.0.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.12.2)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.1.43)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (6.3.3)\n",
            "Collecting watchdog<6,>=2.1.5 (from streamlit)\n",
            "  Downloading watchdog-5.0.3-py3-none-manylinux2014_x86_64.whl.metadata (41 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m41.9/41.9 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting SQLAlchemy<2.0.36,>=1.4 (from langchain_community)\n",
            "  Downloading SQLAlchemy-2.0.35-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (3.10.10)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting httpx-sse<0.5.0,>=0.4.0 (from langchain_community)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting langchain<0.4.0,>=0.3.6 (from langchain_community)\n",
            "  Downloading langchain-0.3.7-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.1.137)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain_community)\n",
            "  Downloading pydantic_settings-2.6.1-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting build>=1.0.3 (from chromadb)\n",
            "  Downloading build-1.2.2.post1-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.10/dist-packages (from chromadb) (2.9.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install flask pymongo ngrok pyngrok langchain_groq streamlit PyPDF2 langchain_community chromadb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import os\n",
        "import tempfile\n",
        "import time\n",
        "from datetime import datetime\n",
        "import pkg_resources\n",
        "import streamlit as st\n",
        "import PyPDF2\n",
        "from langchain_groq import ChatGroq\n",
        "\n",
        "# Function to safely handle rerun based on Streamlit version\n",
        "def safe_rerun():\n",
        "    \"\"\"Safely rerun the app based on Streamlit version\"\"\"\n",
        "    try:\n",
        "        st_version = pkg_resources.get_distribution('streamlit').version\n",
        "        if pkg_resources.parse_version(st_version) >= pkg_resources.parse_version('1.27.0'):\n",
        "            st.rerun()\n",
        "        else:\n",
        "            st.experimental_rerun()\n",
        "    except Exception as e:\n",
        "        try:\n",
        "            st.rerun()\n",
        "        except:\n",
        "            try:\n",
        "                st.experimental_rerun()\n",
        "            except:\n",
        "                st.error(\"Unable to refresh the page. Please refresh manually (F5 or Ctrl+R)\")\n",
        "\n",
        "# Set page configuration\n",
        "st.set_page_config(\n",
        "    page_title=\"Medical Document Analyzer\",\n",
        "    page_icon=\"üè•\",\n",
        "    layout=\"wide\",\n",
        "    initial_sidebar_state=\"expanded\"\n",
        ")\n",
        "\n",
        "# Custom CSS for better UI and chat visibility\n",
        "st.markdown(\"\"\"\n",
        "    <style>\n",
        "    .main {\n",
        "        padding: 2rem;\n",
        "    }\n",
        "    .stButton > button {\n",
        "        width: 100%;\n",
        "        border-radius: 5px;\n",
        "        height: 3em;\n",
        "    }\n",
        "    .chat-message {\n",
        "        padding: 1.5rem;\n",
        "        border-radius: 0.5rem;\n",
        "        margin-bottom: 1rem;\n",
        "        display: flex;\n",
        "        flex-direction: column;\n",
        "        box-shadow: 0 2px 4px rgba(0,0,0,0.1);\n",
        "    }\n",
        "    .chat-message.user {\n",
        "        background-color: #2196F3;\n",
        "        color: white;\n",
        "    }\n",
        "    .chat-message.user strong {\n",
        "        color: #E3F2FD;\n",
        "    }\n",
        "    .chat-message.assistant {\n",
        "        background-color: #F5F5F5;\n",
        "        color: #333333;\n",
        "        border-left: 4px solid #2196F3;\n",
        "    }\n",
        "    .chat-message.assistant strong {\n",
        "        color: #1976D2;\n",
        "    }\n",
        "    .file-uploader {\n",
        "        padding: 2rem;\n",
        "        border: 2px dashed #ccc;\n",
        "        border-radius: 10px;\n",
        "        text-align: center;\n",
        "    }\n",
        "    .stTextInput>div>div>input {\n",
        "        background-color: white;\n",
        "        color: #333333;\n",
        "    }\n",
        "    </style>\n",
        "    \"\"\", unsafe_allow_html=True)\n",
        "\n",
        "# Initialize session state\n",
        "if 'patient_records' not in st.session_state:\n",
        "    st.session_state.patient_records = {}\n",
        "if 'llm' not in st.session_state:\n",
        "    # Initialize the LLM\n",
        "    os.environ['GROQ_API_KEY'] = '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'\n",
        "    st.session_state.llm = ChatGroq(\n",
        "        model=\"llama-3.1-70b-versatile\",\n",
        "        temperature=0,\n",
        "        max_tokens=8000,\n",
        "        timeout=None,\n",
        "        max_retries=2\n",
        "    )\n",
        "\n",
        "def extract_text_from_pdf(pdf_file):\n",
        "    \"\"\"Extract text from a PDF file.\"\"\"\n",
        "    try:\n",
        "        pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
        "        text = \"\"\n",
        "        for page in pdf_reader.pages:\n",
        "            text += page.extract_text()\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error extracting text from PDF: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "def generate_summary(text):\n",
        "    \"\"\"Generate a summary using the LLM.\"\"\"\n",
        "    try:\n",
        "        messages = [\n",
        "            (\"system\", \"\"\"You are an AI medical assistant specialized in analyzing medical documents.\n",
        "            Please provide a concise summary focusing on key medical findings, diagnoses,\n",
        "            treatments, and recommendations. Use medical terminology appropriately.\"\"\"),\n",
        "            (\"human\", text)\n",
        "        ]\n",
        "        response = st.session_state.llm.invoke(messages)\n",
        "        return response.content\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error generating summary: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "def process_uploaded_file(uploaded_file):\n",
        "    \"\"\"Process a single uploaded PDF file.\"\"\"\n",
        "    if uploaded_file.type != \"application/pdf\":\n",
        "        st.error(f\"{uploaded_file.name} is not a PDF file.\")\n",
        "        return None\n",
        "\n",
        "    with tempfile.NamedTemporaryFile(delete=False) as temp_file:\n",
        "        temp_file.write(uploaded_file.getvalue())\n",
        "        text = extract_text_from_pdf(temp_file.name)\n",
        "        os.unlink(temp_file.name)\n",
        "\n",
        "        if text:\n",
        "            summary = generate_summary(text)\n",
        "            if summary:\n",
        "                return {\n",
        "                    'filename': uploaded_file.name,\n",
        "                    'text': text,\n",
        "                    'summary': summary,\n",
        "                    'timestamp': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "                }\n",
        "    return None\n",
        "\n",
        "def chat_with_docs(question, context):\n",
        "    \"\"\"Chat with the documents using the LLM.\"\"\"\n",
        "    try:\n",
        "        messages = [\n",
        "            (\"system\", \"\"\"You are an AI medical assistant. Use the provided medical document\n",
        "            context to answer questions. If the answer cannot be found in the context,\n",
        "            clearly state that. Always maintain medical accuracy and professionalism.\"\"\"),\n",
        "            (\"human\", f\"Context: {context}\\n\\nQuestion: {question}\")\n",
        "        ]\n",
        "        response = st.session_state.llm.invoke(messages)\n",
        "        return response.content\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error processing question: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "# Sidebar\n",
        "with st.sidebar:\n",
        "    st.image(\"https://via.placeholder.com/150?text=Medical+Logo\", width=150)\n",
        "    st.title(\"Patient Records\")\n",
        "\n",
        "    # File uploader\n",
        "    uploaded_file = st.file_uploader(\n",
        "        \"Upload Patient Record (PDF)\",\n",
        "        type=[\"pdf\"],\n",
        "        accept_multiple_files=False\n",
        "    )\n",
        "\n",
        "    if uploaded_file:\n",
        "        if st.button(\"Process Record\"):\n",
        "            with st.spinner(\"Processing record...\"):\n",
        "                patient_record = process_uploaded_file(uploaded_file)\n",
        "                if patient_record:\n",
        "                    st.session_state.patient_records[uploaded_file.name] = patient_record\n",
        "                    st.success(f\"Processed: {uploaded_file.name}\")\n",
        "\n",
        "    # Show processed files\n",
        "    if st.session_state.patient_records:\n",
        "        st.subheader(\"Patient Records\")\n",
        "        for filename in st.session_state.patient_records:\n",
        "            st.text(f\"üìÑ {filename}\")\n",
        "\n",
        "# Main content area\n",
        "st.title(\"Medical Document Analyzer\")\n",
        "\n",
        "# Tabs for different views\n",
        "tab1, tab2 = st.tabs([\"Patient Summaries\", \"Chat Interface\"])\n",
        "\n",
        "with tab1:\n",
        "    if st.session_state.patient_records:\n",
        "        for filename, data in st.session_state.patient_records.items():\n",
        "            with st.expander(f\"Summary: {filename}\"):\n",
        "                st.write(f\"**Processed on:** {data['timestamp']}\")\n",
        "                st.write(\"**Summary:**\")\n",
        "                st.write(data['summary'])\n",
        "\n",
        "                if st.button(f\"View Full Text ({filename})\", key=f\"full_text_{filename}\"):\n",
        "                    st.write(\"**Full Text:**\")\n",
        "                    st.text_area(\"\", data['text'], height=300, key=f\"text_{filename}\")\n",
        "    else:\n",
        "        st.info(\"Upload and process patient records to see summaries here.\")\n",
        "\n",
        "with tab2:\n",
        "    st.subheader(\"Chat with Patient Records\")\n",
        "\n",
        "    if st.session_state.patient_records:\n",
        "        # Display chat history\n",
        "        if 'chat_history' not in st.session_state:\n",
        "            st.session_state.chat_history = []\n",
        "\n",
        "        for message in st.session_state.chat_history:\n",
        "            with st.container():\n",
        "                st.markdown(f\"\"\"\n",
        "                    <div class=\"chat-message {message['role']}\">\n",
        "                        <div><strong>{'You' if message['role'] == 'user' else 'Assistant'}:</strong></div>\n",
        "                        <div style=\"margin-top: 0.5rem;\">{message['content']}</div>\n",
        "                    </div>\n",
        "                    \"\"\", unsafe_allow_html=True)\n",
        "\n",
        "        # Chat input with error handling\n",
        "        question = st.text_input(\"Ask a question about your patient records:\", key=\"chat_input\")\n",
        "        if question:\n",
        "            if st.button(\"Send\", key=\"send_button\"):\n",
        "                # Combine all patient record contexts\n",
        "                context = \"\\n\\n\".join([\n",
        "                    f\"Patient Record: {filename}\\n{data['text']}\"\n",
        "                    for filename, data in st.session_state.patient_records.items()\n",
        "                ])\n",
        "\n",
        "                with st.spinner(\"Thinking...\"):\n",
        "                    response = chat_with_docs(question, context)\n",
        "\n",
        "                if response:\n",
        "                    # Add to chat history\n",
        "                    st.session_state.chat_history.append({\n",
        "                        'role': 'user',\n",
        "                        'content': question\n",
        "                    })\n",
        "                    st.session_state.chat_history.append({\n",
        "                        'role': 'assistant',\n",
        "                        'content': response\n",
        "                    })\n",
        "                    # Use the safe rerun function\n",
        "                    safe_rerun()\n",
        "    else:\n",
        "        st.info(\"Upload and process patient records to start chatting.\")\n",
        "\n",
        "# Footer\n",
        "st.markdown(\"---\")\n",
        "st.markdown(\n",
        "    \"Made with ‚ù§Ô∏è for healthcare professionals. \"\n",
        "    \"Upload medical documents securely and get AI-powered insights.\"\n",
        ")"
      ],
      "metadata": {
        "id": "p070l8SILTWy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q -O - ipv4.icanhazip.com"
      ],
      "metadata": {
        "id": "YxmPHLrzLdPI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run app.py & npx localtunnel --port 8501"
      ],
      "metadata": {
        "id": "2nJdtD4IMlg1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vPoEPk-LJCqn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}